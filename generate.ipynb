{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8067fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ae89fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     797 train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "08830adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     750\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v '^#' train.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "529f5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat test.txt train.txt > all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "941b21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1888\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   110K May  8 12:02 all.txt\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   108K May  8 12:01 all_no_comments.txt\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   7.8K May  8 12:02 generate.ipynb\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   103K May  8 12:01 good.txt\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   113K May  8 11:07 good_1.txt\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   219K May  8 11:59 good_converted.json\r\n",
      "-rw-r--r--@ 1 hanscfa  staff   1.5K May  8 10:07 kommaregler.json\r\n",
      "-rw-r--r--@ 1 hanscfa  staff    15K May  8 12:00 test.txt\r\n",
      "-rw-r--r--@ 1 hanscfa  staff    96K May  8 12:02 train.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "895bb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -v '^#' all.txt > all_no_comments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cbb91bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     990 all.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "77869b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     910 all_no_comments.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l all_no_comments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ed3ec04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      70\r\n"
     ]
    }
   ],
   "source": [
    "! grep '> 99$' all_no_comments.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "141c9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -v '> 99$' all_no_comments.txt > good.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "89574fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En vinter <KOMMA> det må ha vært i 1990 <KOMMA> bodde han i USA. > 503\r\n",
      "Lov om petroleumsvirksomhet <KOMMA> lov 29. november 1996 nr. 72 <KOMMA> fastslår at staten har eksklusiv rett til ressursforvaltning. > 501\r\n",
      "Alle studiene i boken er gjennomført i perioden 2000–2006 <KOMMA> altså i årene etter den asiatiske finanskrisen i 1997 <KOMMA> og inkluderer studier fra Thailand. > 502\r\n"
     ]
    }
   ],
   "source": [
    "! grep '99' good.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c18a4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     840 good.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l good.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cf1a11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file written to good_converted.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Replace 'filename.txt' with the path to your file\n",
    "input_filename = 'good.txt'\n",
    "output_filename = input_filename.rsplit('.', 1)[0] + '_converted.json'\n",
    "\n",
    "# Function to process each line based on the rules you provided\n",
    "def process_line(first_part, second_part):\n",
    "    # Initialize correct and wrong versions of the first part\n",
    "    correct_sentence, wrong_sentence = first_part, second_part\n",
    "    category = None\n",
    "\n",
    "    # Apply the ' <KOMMA>' rule\n",
    "    if ' <KOMMA>' in first_part:\n",
    "        correct_sentence = first_part.replace(' <KOMMA>', ',')\n",
    "        wrong_sentence = first_part.replace(' <KOMMA>', '')\n",
    "\n",
    "    # Apply the ' <IKKE>' rule\n",
    "    elif ' <IKKE>' in first_part:\n",
    "        correct_sentence = first_part.replace(' <IKKE>', '')\n",
    "        wrong_sentence = first_part.replace(' <IKKE>', ',')\n",
    "\n",
    "    # Extract category as an integer\n",
    "    try:\n",
    "        category = int(second_part.strip())\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Unable to convert '{second_part.strip()}' to an integer.\")\n",
    "\n",
    "    # Create a dictionary object for the JSON output\n",
    "    return {\n",
    "        \"correct\": correct_sentence,\n",
    "        \"wrong\": wrong_sentence,\n",
    "        \"category\": category\n",
    "    }\n",
    "\n",
    "# Open the input file and process each line\n",
    "with open(input_filename, 'r') as infile, open(output_filename, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Split at the first occurrence of '>'\n",
    "        parts = line.split(' > ', 1)\n",
    "        if len(parts) < 2:\n",
    "            print(f\"Warning: No '>' found in line: {line}\")\n",
    "            continue\n",
    "\n",
    "        # Extract and strip the first and second parts\n",
    "        first_part = parts[0].strip()\n",
    "        second_part = parts[1].strip()\n",
    "\n",
    "        # Process the line using the defined rules\n",
    "        json_object = process_line(first_part, second_part)\n",
    "\n",
    "        # Serialize the object to a JSON string with double quotes\n",
    "        json_string = json.dumps(json_object, ensure_ascii=False)\n",
    "\n",
    "        # Write the JSON string to the output file\n",
    "        outfile.write(f\"{json_string}\\n\")\n",
    "\n",
    "print(f\"Processed file written to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5823def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface[cli]\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "\u001b[33mWARNING: huggingface 0.0.1 does not provide the extra 'cli'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp311-cp311-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: huggingface, xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-0.0.1 huggingface-hub-0.23.0 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"huggingface[cli]\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49f222f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eva klatrer, og Ola går på ski.</td>\n",
       "      <td>Eva klatrer og Ola går på ski.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trine klatrer, Jens sykler, og Ola går på ski.</td>\n",
       "      <td>Trine klatrer Jens sykler og Ola går på ski.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skjuler han seg, eller har han flyktet, eller ...</td>\n",
       "      <td>Skjuler han seg eller har han flyktet eller er...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             correct  \\\n",
       "0                    Eva klatrer, og Ola går på ski.   \n",
       "1     Trine klatrer, Jens sykler, og Ola går på ski.   \n",
       "2  Skjuler han seg, eller har han flyktet, eller ...   \n",
       "\n",
       "                                               wrong  category  \n",
       "0                     Eva klatrer og Ola går på ski.         1  \n",
       "1       Trine klatrer Jens sykler og Ola går på ski.         1  \n",
       "2  Skjuler han seg eller har han flyktet eller er...         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ncb = pd.read_json(\"ncb.json\", lines=True)\n",
    "ncb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794b46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(\n",
    "    train=Dataset.from_pandas(ncb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d681604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/hanscfa/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e181f055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207e393fdb2d4ad7a6c94476e93e443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5236ce928f64ca0a13033e815b9a628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hcfa/ncb/commit/c1d931193ec7174e84b67ec0645e37b7069505ac', commit_message='Upload dataset', commit_description='', oid='c1d931193ec7174e84b67ec0645e37b7069505ac', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_name = \"hcfa\" # your HF profile name\n",
    "dataset_name = \"ncb\" # or change the name if you want\n",
    "private = False # this means the dataset will be publicly available. otherwise, you can make it private and share with specific HF users\n",
    "\n",
    "dataset.push_to_hub(f\"{profile_name}/{dataset_name}\", private=private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95f844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
